<!DOCTYPE html>
<html>
<head>
    <title>Sports vs Politics Text Classification</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

<div class="container">

<h1>Sports vs Politics Text Classification</h1>
<p class="subtitle">Natural Language Understanding Assignment</p>
<p><b>Author:</b> Vishal | Roll No: B23CM1048</p>

<!-- ================================================= -->
<div class="card">
<h2>Abstract</h2>
<p>
This project presents a systematic comparative study of classical machine learning 
approaches for binary news text classification. The objective was to classify news 
articles into <b>Sports</b> and <b>Politics</b> categories using two datasets with 
distinct characteristics: the BBC News dataset (clean, well-structured) and the 
AG News dataset (large-scale and diverse).
</p>

<p>
The study evaluates model performance, robustness, and generalization across 
datasets that differ in size, lexical overlap, writing style, and domain variability. 
Three supervised learning models â€” Multinomial Naive Bayes, Logistic Regression, 
and Linear Support Vector Machine â€” were trained using TF-IDF features. 
Performance was analyzed using accuracy, precision, recall, and F1-score.
</p>
</div>

<!-- ================================================= -->
<div class="card">
<h2>ðŸ“‚ Dataset Description & Collection</h2>

<h3>1. BBC News Dataset</h3>
<p>
The BBC dataset contains professionally written news articles organized in 
separate directories by topic. Only two categories were selected: Sports and Politics.
</p>

<ul>
<li>Total Articles: 928</li>
<li>Sports: 511 articles</li>
<li>Politics: 417 articles</li>
<li>Format: Plain text files</li>
</ul>

<p>
This dataset is relatively clean, well-separated, and domain-specific, making 
it suitable for controlled experimentation.
</p>

<h3>2. AG News Dataset</h3>
<p>
The AG News dataset is a large-scale news corpus in CSV format containing 
four categories. For this study, only:
</p>

<ul>
<li>Class 1 (World) â†’ Relabelled as Politics</li>
<li>Class 2 (Sports)</li>
</ul>

<p>
Approximately 11,400 filtered samples were used. Articles consist of short 
titles and descriptions, increasing lexical ambiguity and classification complexity.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Comparative Dataset Analysis</h2>

<p>
The BBC dataset demonstrates strong lexical separation between domains. 
Sports articles contain domain-specific terminology such as 
<i>"match", "league", "coach", "season"</i>, whereas political articles 
include terms like <i>"government", "election", "minister", "policy"</i>.
</p>

<p>
In contrast, AG News articles are shorter and exhibit higher vocabulary overlap. 
For example, words such as <i>"team", "campaign", "leader"</i> may appear in both 
sports and political contexts. This overlap increases decision boundary complexity.
</p>

<p>
Therefore:
</p>

<ul>
<li>BBC represents a <b>clean, separable classification scenario</b>.</li>
<li>AG News represents a <b>real-world, noisy classification scenario</b>.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Methodology</h2>

<h3>1. Preprocessing</h3>
<ul>
<li>Lowercasing to normalize vocabulary.</li>
<li>Stopword removal to eliminate high-frequency non-informative words.</li>
<li>No stemming/lemmatization applied to preserve interpretability.</li>
</ul>

<h3>2. Feature Engineering: TF-IDF</h3>
<p>
Text documents were transformed into numerical feature vectors using 
Term Frequencyâ€“Inverse Document Frequency (TF-IDF).
</p>

<p>
TF-IDF assigns higher weight to words that are frequent within a document 
but rare across the corpus. This reduces the dominance of common words 
and enhances discriminative terms.
</p>

<h3>3. Models Evaluated</h3>
<ul>
<li><b>Multinomial Naive Bayes:</b> Probabilistic generative model assuming conditional independence.</li>
<li><b>Logistic Regression:</b> Linear discriminative model optimizing log-loss.</li>
<li><b>Linear SVM:</b> Margin-based classifier maximizing class separation.</li>
</ul>

<h3>4. Evaluation Strategy</h3>
<ul>
<li>BBC Dataset â†’ 80â€“20 Stratified Train-Test Split</li>
<li>AG News â†’ Predefined Train/Test Split</li>
<li>Metrics â†’ Accuracy, Precision, Recall, F1-score</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Experimental Results</h2>

<h3>BBC Dataset Results</h3>
<table>
<tr><th>Model</th><th>Accuracy</th></tr>
<tr><td>Naive Bayes</td><td>1.000</td></tr>
<tr><td>Logistic Regression</td><td>1.000</td></tr>
<tr><td>SVM</td><td>1.000</td></tr>
</table>

<p>
All models achieved perfect accuracy due to strong lexical separation 
between categories. The decision boundary is nearly linearly separable 
in TF-IDF space.
</p>

<h3>AG News Dataset Results</h3>
<table>
<tr><th>Model</th><th>Accuracy</th></tr>
<tr><td>Naive Bayes</td><td>0.9739</td></tr>
<tr><td>Logistic Regression</td><td>0.9774</td></tr>
<tr><td>SVM</td><td>0.9761</td></tr>
</table>

<p>
Performance slightly decreased compared to BBC due to:
</p>

<ul>
<li>Shorter article length</li>
<li>Vocabulary overlap</li>
<li>Context ambiguity</li>
</ul>

<p>
Logistic Regression achieved the highest accuracy, suggesting that 
discriminative models better handle overlapping feature distributions.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Model Comparison Chart</h2>
<img src="assets/comparison.png" alt="Model Comparison Chart" class="chart">
</div>

<div class="card">
<h2>Precision & Recall Analysis</h2>
<img src="assets/Precision-Recall.png" alt="Precision & Recall" class="chart">
</div>

<!-- ================================================= -->
<div class="card">
<h2>Discussion & Interpretation</h2>

<p>
The results demonstrate that classical machine learning models remain 
highly effective for structured topic classification tasks. 
However, dataset characteristics significantly impact performance.
</p>

<ul>
<li>BBC dataset confirms near-linear separability.</li>
<li>AG dataset demonstrates real-world ambiguity challenges.</li>
<li>Logistic Regression slightly outperforms due to discriminative optimization.</li>
<li>SVM performs consistently strong due to margin maximization.</li>
</ul>

<p>
The minimal difference between models suggests that feature representation 
(TF-IDF) plays a more critical role than classifier complexity for this task.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Challenges & Limitations</h2>

<ul>
<li>TF-IDF ignores semantic similarity and contextual meaning.</li>
<li>No word embeddings used (semantic relationships not captured).</li>
<li>Binary classification oversimplifies multi-topic articles.</li>
<li>No deep contextual modeling (e.g., transformers).</li>
<li>Domain shift between datasets affects generalization.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Future Improvements</h2>

<ul>
<li>Incorporate Word Embeddings (Word2Vec, GloVe).</li>
<li>Apply Transformer models (BERT, RoBERTa).</li>
<li>Use Cross-Validation for stronger evaluation.</li>
<li>Perform Error Analysis with confusion matrices.</li>
<li>Extend to multi-class classification.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Conclusion</h2>

<p>
This comparative study demonstrates that traditional linear classifiers 
combined with TF-IDF remain strong baselines for news topic classification. 
While performance is near-perfect in clean datasets, real-world data introduces 
ambiguity that slightly reduces accuracy.
</p>

<p>
The findings emphasize the importance of dataset characteristics, feature 
engineering, and model selection when designing text classification systems.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Project Resources</h2>
<p>
<a href="Report.pdf" target="_blank">View Full Report (PDF)</a>
</p>
<p>
<a href="https://github.com/Vishaldhaniya08/Sports-vs-Politics-Text-Classification" target="_blank">
View Source Code on GitHub
</a>
</p>
</div>

<footer>
<p>Â© 2026 Vishal | Natural Language Understanding Assignment</p>
</footer>

</div>
</body>
</html>
