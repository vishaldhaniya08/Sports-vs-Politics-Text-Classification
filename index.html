<!DOCTYPE html>
<html>
<head>
    <title>Sports vs Politics Text Classification</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

<div class="container">

<h1>Sports vs Politics Text Classification</h1>
<p class="subtitle">Natural Language Understanding Assignment</p>
<p><b>Author:</b> Vishal | Roll No: B23CM1048</p>

<!-- ================================================= -->
<div class="card">
<h2>Abstract</h2>
<p>
This project presents a systematic comparative study of classical machine learning 
approaches for binary news text classification. The objective was to classify news 
articles into <b>Sports</b> and <b>Politics</b> categories using two datasets with 
distinct characteristics: the BBC News dataset (clean, well-structured) and the 
AG News dataset (large-scale and diverse).
</p>

<p>
The study evaluates model performance, robustness, and generalization across 
datasets that differ in size, lexical overlap, writing style, and domain variability. 
Three supervised learning models ‚Äî Multinomial Naive Bayes, Logistic Regression, 
and Linear Support Vector Machine ‚Äî were trained using TF-IDF features. 
Performance was analyzed using accuracy, precision, recall, and F1-score.
</p>
</div>

<!-- ================================================= -->
<div class="card">
<h2>üìÇ Dataset Description & Collection</h2>

<h3>1. BBC News Dataset</h3>
<p>
The BBC dataset contains professionally written news articles organized in 
separate directories by topic. Only two categories were selected: Sports and Politics.
</p>

<ul>
<li>Total Articles: 928</li>
<li>Sports: 511 articles</li>
<li>Politics: 417 articles</li>
<li>Format: Plain text files</li>
</ul>

<p>
This dataset is relatively clean, well-separated, and domain-specific, making 
it suitable for controlled experimentation.
</p>

<h3>2. AG News Dataset</h3>
<p>
The AG News dataset is a large-scale news corpus in CSV format containing 
four categories. For this study, only:
</p>

<ul>
<li>Class 1 (World) ‚Üí Relabelled as Politics</li>
<li>Class 2 (Sports)</li>
</ul>

<p>
Approximately 11,400 filtered samples were used. Articles consist of short 
titles and descriptions, increasing lexical ambiguity and classification complexity.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Comparative Dataset Analysis</h2>

<p>
The BBC dataset demonstrates strong lexical separation between domains. 
Sports articles contain domain-specific terminology such as 
<i>"match", "league", "coach", "season"</i>, whereas political articles 
include terms like <i>"government", "election", "minister", "policy"</i>.
</p>

<p>
In contrast, AG News articles are shorter and exhibit higher vocabulary overlap. 
For example, words such as <i>"team", "campaign", "leader"</i> may appear in both 
sports and political contexts. This overlap increases decision boundary complexity.
</p>

<p>
Therefore:
</p>

<ul>
<li>BBC represents a <b>clean, separable classification scenario</b>.</li>
<li>AG News represents a <b>real-world, noisy classification scenario</b>.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Methodology</h2>

<h3>1. Text Preprocessing</h3>
<p>
Text preprocessing is a crucial step in any Natural Language Processing pipeline. 
Raw textual data often contains inconsistencies, noise, and non-informative tokens 
that may negatively impact model performance.
</p>

<ul>
<li><b>Lowercasing:</b> All text was converted to lowercase to ensure that words such as 
"Government" and "government" are treated as identical features.</li>

<li><b>Stopword Removal:</b> Common words such as "the", "is", and "and" were removed 
using built-in English stopword lists. These words carry little discriminative value 
for topic classification.</li>

<li><b>No Stemming/Lemmatization:</b> Stemming was intentionally not applied in order 
to preserve interpretability and avoid distortion of meaningful domain-specific words 
(e.g., "election" vs "elect").</li>
</ul>

<p>
These preprocessing steps reduce vocabulary size and improve computational efficiency 
while maintaining important domain signals.
</p>

<hr>

<h3>2. Feature Extraction: TF-IDF Representation</h3>

<p>
Text documents cannot be directly processed by machine learning algorithms. 
Therefore, they must be converted into numerical feature vectors. 
This study uses <b>Term Frequency‚ÄìInverse Document Frequency (TF-IDF)</b>.
</p>

<p>
TF-IDF balances two important aspects:
</p>

<ul>
<li><b>Term Frequency (TF):</b> Measures how often a word appears in a document.</li>
<li><b>Inverse Document Frequency (IDF):</b> Reduces weight of words that appear in many documents.</li>
</ul>

<p>
Mathematically:
</p>

<p>
TF-IDF(word) = TF(word, document) √ó log(Total Documents / Documents containing word)
</p>

<p>
Advantages of TF-IDF:
</p>

<ul>
<li>Reduces dominance of very common words.</li>
<li>Emphasizes discriminative topic-specific terms.</li>
<li>Produces sparse, high-dimensional representations suitable for linear models.</li>
</ul>

<p>
However, TF-IDF does not capture semantic relationships or word order.
</p>

<hr>

<h3>3. Models Evaluated</h3>

<h4>3.1 Multinomial Naive Bayes (Generative Model)</h4>

<p>
Naive Bayes is a probabilistic classifier based on Bayes' Theorem with a 
conditional independence assumption between features.
</p>

<p>
It calculates:
</p>

<p>
P(Class | Document) ‚àù P(Class) √ó Œ† P(Word | Class)
</p>

<p>
Advantages:
</p>

<ul>
<li>Extremely fast and efficient.</li>
<li>Performs well on high-dimensional sparse data.</li>
<li>Strong baseline for text classification.</li>
</ul>

<p>
Limitations:
</p>

<ul>
<li>Assumes word independence (often unrealistic).</li>
<li>Struggles when features are highly correlated.</li>
</ul>

<p>
In this project, Naive Bayes performs perfectly on BBC but slightly lower on AG News, 
indicating sensitivity to overlapping vocabulary.
</p>

<hr>

<h4>3.2 Logistic Regression (Discriminative Linear Model)</h4>

<p>
Logistic Regression is a linear classifier that directly models the posterior probability 
P(Class | Features) using a sigmoid function.
</p>

<p>
It optimizes log-loss (cross-entropy loss) to find the best decision boundary.
</p>

<p>
Advantages:
</p>

<ul>
<li>No independence assumption.</li>
<li>Handles correlated features better than Naive Bayes.</li>
<li>Produces well-calibrated probabilities.</li>
</ul>

<p>
Limitations:
</p>

<ul>
<li>Still linear ‚Äî cannot capture non-linear relationships.</li>
<li>Performance depends heavily on feature representation.</li>
</ul>

<p>
Logistic Regression achieved the highest accuracy on AG News, suggesting 
better robustness in noisy and overlapping scenarios.
</p>

<hr>

<h4>3.3 Support Vector Machine (Margin-Based Classifier)</h4>

<p>
Linear Support Vector Machine (SVM) attempts to find the optimal hyperplane 
that maximizes the margin between classes.
</p>

<p>
Rather than modeling probabilities, SVM focuses on maximizing class separation.
</p>

<p>
Advantages:
</p>

<ul>
<li>Highly effective in high-dimensional sparse spaces.</li>
<li>Robust against overfitting in text classification tasks.</li>
<li>Often outperforms probabilistic models in NLP tasks.</li>
</ul>

<p>
Limitations:
</p>

<ul>
<li>Does not provide probability outputs by default.</li>
<li>Can be computationally expensive for very large datasets.</li>
</ul>

<p>
SVM performed very competitively across both datasets, confirming 
its suitability for text classification.
</p>

<hr>

<h3>4. Model Comparison & Justification</h3>

<table>
<tr>
<th>Model</th>
<th>Type</th>
<th>Strength</th>
<th>Weakness</th>
</tr>

<tr>
<td>Naive Bayes</td>
<td>Generative</td>
<td>Fast, simple, strong baseline</td>
<td>Independence assumption</td>
</tr>

<tr>
<td>Logistic Regression</td>
<td>Discriminative</td>
<td>Handles correlated features well</td>
<td>Linear boundary</td>
</tr>

<tr>
<td>SVM</td>
<td>Margin-based</td>
<td>Strong separation in sparse space</td>
<td>No probability output</td>
</tr>
</table>

<p>
Overall, results indicate that:
</p>

<ul>
<li>When classes are clearly separable (BBC), all linear models perform equally well.</li>
<li>When lexical overlap increases (AG News), discriminative models (Logistic Regression, SVM) show slight advantages.</li>
<li>Feature representation (TF-IDF) plays a critical role in overall performance.</li>
</ul>

<hr>

<h3>5. Evaluation Strategy</h3>

<ul>
<li><b>BBC Dataset:</b> 80‚Äì20 stratified train-test split to maintain class balance.</li>
<li><b>AG News:</b> Predefined train-test split used to ensure standardized evaluation.</li>
<li><b>Metrics:</b> Accuracy, Precision, Recall, F1-score.</li>
</ul>

<p>
Accuracy provides overall correctness, while Precision and Recall evaluate 
class-specific performance. F1-score balances both, especially important 
in slightly imbalanced datasets.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Experimental Results</h2>

<h3>BBC Dataset Results</h3>
<table>
<tr><th>Model</th><th>Accuracy</th></tr>
<tr><td>Naive Bayes</td><td>1.000</td></tr>
<tr><td>Logistic Regression</td><td>1.000</td></tr>
<tr><td>SVM</td><td>1.000</td></tr>
</table>

<p>
All models achieved perfect accuracy due to strong lexical separation 
between categories. The decision boundary is nearly linearly separable 
in TF-IDF space.
</p>

<h3>AG News Dataset Results</h3>
<table>
<tr><th>Model</th><th>Accuracy</th></tr>
<tr><td>Naive Bayes</td><td>0.9739</td></tr>
<tr><td>Logistic Regression</td><td>0.9774</td></tr>
<tr><td>SVM</td><td>0.9761</td></tr>
</table>

<p>
Performance slightly decreased compared to BBC due to:
</p>

<ul>
<li>Shorter article length</li>
<li>Vocabulary overlap</li>
<li>Context ambiguity</li>
</ul>

<p>
Logistic Regression achieved the highest accuracy, suggesting that 
discriminative models better handle overlapping feature distributions.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Model Comparison Chart</h2>
<img src="assets/comparison.png" alt="Model Comparison Chart" class="chart">
</div>

<div class="card">
<h2>Precision & Recall Analysis</h2>
<img src="assets/Precision-Recall.png" alt="Precision & Recall" class="chart">
</div>

<!-- ================================================= -->
<div class="card">
<h2>Discussion & Interpretation</h2>

<p>
The results demonstrate that classical machine learning models remain 
highly effective for structured topic classification tasks. 
However, dataset characteristics significantly impact performance.
</p>

<ul>
<li>BBC dataset confirms near-linear separability.</li>
<li>AG dataset demonstrates real-world ambiguity challenges.</li>
<li>Logistic Regression slightly outperforms due to discriminative optimization.</li>
<li>SVM performs consistently strong due to margin maximization.</li>
</ul>

<p>
The minimal difference between models suggests that feature representation 
(TF-IDF) plays a more critical role than classifier complexity for this task.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Challenges & Limitations</h2>

<ul>
<li>TF-IDF ignores semantic similarity and contextual meaning.</li>
<li>No word embeddings used (semantic relationships not captured).</li>
<li>Binary classification oversimplifies multi-topic articles.</li>
<li>No deep contextual modeling (e.g., transformers).</li>
<li>Domain shift between datasets affects generalization.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Future Improvements</h2>

<ul>
<li>Incorporate Word Embeddings (Word2Vec, GloVe).</li>
<li>Apply Transformer models (BERT, RoBERTa).</li>
<li>Use Cross-Validation for stronger evaluation.</li>
<li>Perform Error Analysis with confusion matrices.</li>
<li>Extend to multi-class classification.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Conclusion</h2>

<p>
This comparative study demonstrates that traditional linear classifiers 
combined with TF-IDF remain strong baselines for news topic classification. 
While performance is near-perfect in clean datasets, real-world data introduces 
ambiguity that slightly reduces accuracy.
</p>

<p>
The findings emphasize the importance of dataset characteristics, feature 
engineering, and model selection when designing text classification systems.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Project Resources</h2>
<p>
<a href="Report.pdf" target="_blank">View Full Report (PDF)</a>
</p>
<p>
<a href="https://github.com/Vishaldhaniya08/Sports-vs-Politics-Text-Classification" target="_blank">
View Source Code on GitHub
</a>
</p>
</div>

<footer>
<p>¬© 2026 Vishal | Natural Language Understanding Assignment</p>
</footer>

</div>
</body>
</html>
