<!DOCTYPE html>
<html>
<head>
    <title>Sports vs Politics Text Classification</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>

<div class="container">

<h1>Sports vs Politics Text Classification</h1>
<p class="subtitle">Natural Language Understanding Assignment</p>
<p><b>Author:</b> Vishal | Roll No: B23CM1048</p>

<!-- ================================================= -->
<div class="card">
<h2>Abstract</h2>
<p>
This project presents a systematic comparative study of classical machine learning 
approaches for binary news text classification. The objective was to classify news 
articles into <b>Sports</b> and <b>Politics</b> categories using two datasets with 
distinct characteristics: the BBC News dataset (clean, well-structured) and the 
AG News dataset (large-scale and diverse).
</p>

<p>
The study evaluates model performance, robustness, and generalization across 
datasets that differ in size, lexical overlap, writing style, and domain variability. 
Three supervised learning models — Multinomial Naive Bayes, Logistic Regression, 
and Linear Support Vector Machine — were trained using TF-IDF features. 
Performance was analyzed using accuracy, precision, recall, and F1-score.
</p>
</div>

<!-- ================================================= -->
<div class="card">
<h2>Dataset Description & Collection</h2>

<h3>1. BBC News Dataset</h3>

<p>
The BBC News dataset is a well-known benchmark dataset frequently used 
for topic classification tasks. It contains professionally written news 
articles categorized into predefined topic folders.
</p>

<h4>Source & Structure</h4>
<ul>
<li>Source: Publicly available BBC news corpus</li>
<li>Format: Folder-based directory structure</li>
<li>Categories available: Business, Entertainment, Politics, Sport, Tech</li>
</ul>

<p>
For this project, only two categories were selected:
</p>

<ul>
<li><b>Sport</b></li>
<li><b>Politics</b></li>
</ul>

<p>
Each article is stored as an individual text file inside its respective 
category folder. The folder name itself serves as the class label.
</p>

<h4>Dataset Statistics</h4>
<ul>
<li>Total Articles Used: 928</li>
<li>Sports Articles: 511</li>
<li>Politics Articles: 417</li>
<li>Average Article Length: Relatively long (multi-paragraph format)</li>
</ul>

<h4>Why BBC Dataset Was Selected</h4>

<p>
The BBC dataset provides:
</p>

<ul>
<li>Clean, professionally edited text</li>
<li>Strong topical separation</li>
<li>Minimal noise and formatting inconsistencies</li>
<li>Well-defined category boundaries</li>
</ul>

<p>
It serves as an ideal baseline dataset to evaluate classifier behavior 
under structured and controlled conditions.
</p>

<hr>

<h3>2. AG News Dataset</h3>

<p>
The AG News dataset is a large-scale news corpus widely used for 
text classification benchmarking. Unlike BBC, it contains shorter 
news summaries aggregated from multiple news sources.
</p>

<h4>Source & Format</h4>
<ul>
<li>Source: AG News Corpus</li>
<li>Format: CSV file (train.csv and test.csv)</li>
<li>Fields: Class Index, Title, Description</li>
</ul>

<h4>Original Class Distribution</h4>
<ul>
<li>Class 1: World</li>
<li>Class 2: Sports</li>
<li>Class 3: Business</li>
<li>Class 4: Sci/Tech</li>
</ul>

<p>
For this study, the dataset was filtered to include only:
</p>

<ul>
<li><b>Class 1 (World)</b> → Relabelled as <b>Politics</b></li>
<li><b>Class 2 (Sports)</b></li>
</ul>

<p>
Classes 3 and 4 were excluded to maintain binary classification consistency 
with the BBC dataset.
</p>

<h4>Dataset Statistics (Filtered)</h4>
<ul>
<li>Total Filtered Samples: ~11,400</li>
<li>Balanced representation between Sports and Politics</li>
<li>Shorter documents (title + description combined)</li>
</ul>

<h4>Preprocessing Decisions</h4>

<ul>
<li>Title and Description fields were concatenated to enrich textual context.</li>
<li>Class indices were mapped to consistent labels ("sport" and "politics").</li>
<li>Original train-test split provided by AG News was preserved to ensure standardized evaluation.</li>
</ul>

<hr>

<h3>3. Comparative Collection Rationale</h3>

<p>
The selection of these two datasets was intentional and strategic.
</p>

<ul>
<li><b>BBC Dataset</b> represents a clean, well-structured, low-noise environment.</li>
<li><b>AG News Dataset</b> represents a large-scale, diverse, real-world scenario.</li>
</ul>

<p>
By evaluating models on both datasets, the study aims to analyze:
</p>

<ul>
<li>Model behavior under ideal vs realistic conditions</li>
<li>Impact of document length on classification</li>
<li>Influence of vocabulary overlap</li>
<li>Generalization capability across datasets</li>
</ul>

<hr>

<h3>4. Dataset Suitability for Task</h3>

<p>
Both datasets are appropriate for Sports vs Politics classification because:
</p>

<ul>
<li>They contain domain-specific terminology.</li>
<li>They represent real journalistic writing.</li>
<li>They provide sufficient data volume for supervised learning.</li>
<li>They allow evaluation of both separable and overlapping scenarios.</li>
</ul>

<p>
Using both datasets strengthens the experimental design and ensures 
that conclusions are not biased toward a single data source.
</p>

</div>


<!-- ================================================= -->
<div class="card">
<h2>Comparative Data Analysis</h2>

<h3>1. Structural Differences Between Datasets</h3>

<p>
The BBC and AG News datasets differ significantly in structure, scale, and domain diversity. 
These differences directly influence classification complexity and model performance.
</p>

<table>
<tr>
<th>Aspect</th>
<th>BBC Dataset</th>
<th>AG News Dataset</th>
</tr>
<tr>
<td>Size</td>
<td>928 articles</td>
<td>~11,400 filtered samples</td>
</tr>
<tr>
<td>Article Length</td>
<td>Long-form articles</td>
<td>Short title + description</td>
</tr>
<tr>
<td>Domain Coverage</td>
<td>Single-source (BBC)</td>
<td>Multiple news sources</td>
</tr>
<tr>
<td>Vocabulary Overlap</td>
<td>Low overlap</td>
<td>Moderate to High overlap</td>
</tr>
<tr>
<td>Noise Level</td>
<td>Low</td>
<td>Higher (real-world variability)</td>
</tr>
</table>

<hr>

<h3>2. Lexical Distribution Analysis</h3>

<p>
The BBC dataset exhibits strong lexical separation between Sports and Politics categories.
Sports articles frequently contain terms such as:
</p>

<ul>
<li>"match"</li>
<li>"league"</li>
<li>"coach"</li>
<li>"season"</li>
<li>"tournament"</li>
</ul>

<p>
Political articles frequently include:
</p>

<ul>
<li>"government"</li>
<li>"election"</li>
<li>"minister"</li>
<li>"policy"</li>
<li>"parliament"</li>
</ul>

<p>
These clearly distinct vocabularies create near-linear separability in TF-IDF space.
As a result, all models achieved 100% accuracy on the BBC dataset.
</p>

<p>
In contrast, AG News contains shorter texts and more ambiguous terminology. 
Words such as:
</p>

<ul>
<li>"leader"</li>
<li>"campaign"</li>
<li>"team"</li>
<li>"win"</li>
<li>"world"</li>
</ul>

<p>
may appear in both political and sports contexts, increasing feature overlap.
This overlap reduces class separability and increases decision boundary complexity.
</p>

<hr>

<h3>3. Impact of Article Length</h3>

<p>
Longer BBC articles provide richer contextual signals and more discriminative features.
TF-IDF benefits from higher term frequency stability in longer documents.
</p>

<p>
AG News articles are shorter, resulting in:
</p>

<ul>
<li>Lower term frequency stability</li>
<li>Higher sensitivity to ambiguous words</li>
<li>Reduced contextual depth</li>
</ul>

<p>
This explains the slight performance drop (~97%) compared to BBC.
</p>

<hr>

<h3>4. Domain Consistency vs Domain Diversity</h3>

<p>
The BBC dataset is sourced from a single organization, ensuring consistent writing style, 
terminology, and editorial standards. This consistency reduces variability 
within each class.
</p>

<p>
AG News aggregates content from multiple news providers, introducing:
</p>

<ul>
<li>Stylistic variation</li>
<li>Different writing conventions</li>
<li>Mixed domain signals</li>
<li>Greater lexical diversity</li>
</ul>

<p>
Higher intra-class variance makes classification more challenging.
</p>

<hr>

<h3>5. Class Separability in Feature Space</h3>

<p>
In TF-IDF vector space:
</p>

<ul>
<li>BBC dataset shows clear cluster separation.</li>
<li>AG dataset shows partially overlapping clusters.</li>
</ul>

<p>
Linear models perform exceptionally well when classes are linearly separable.
Thus, perfect accuracy on BBC confirms strong linear separability.
</p>

<p>
On AG News, slight overlap means decision boundaries are less clean,
resulting in small classification errors.
</p>

<hr>

<h3>6. Generalization and Robustness</h3>

<p>
The BBC dataset represents a controlled environment ideal for testing model capability 
under clean conditions. However, high performance on BBC does not necessarily imply 
robust real-world generalization.
</p>

<p>
AG News better simulates real-world scenarios, where:
</p>

<ul>
<li>Topic boundaries blur</li>
<li>Language is concise</li>
<li>Domain overlap exists</li>
</ul>

<p>
Therefore, performance on AG News is a stronger indicator of practical robustness.
</p>

<hr>

<h3>7. Why Performance Differs Across Datasets</h3>

<p>
Performance differences arise due to:
</p>

<ul>
<li>Vocabulary overlap</li>
<li>Shorter document length</li>
<li>Higher domain variability</li>
<li>Increased feature ambiguity</li>
</ul>

<p>
Discriminative models such as Logistic Regression and SVM handle 
overlapping feature distributions better than Naive Bayes, explaining 
their slight advantage on AG News.
</p>

<hr>

<h3>8. Key Observations</h3>

<ul>
<li>BBC dataset confirms near-perfect linear separability.</li>
<li>AG dataset introduces realistic classification challenges.</li>
<li>Model performance differences become visible only in noisy conditions.</li>
<li>Feature representation (TF-IDF) remains a critical factor.</li>
</ul>

<p>
Overall, the comparative analysis demonstrates that dataset characteristics 
significantly influence classifier behavior and evaluation outcomes.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Methodology</h2>

<h3>1. Text Preprocessing</h3>
<p>
Text preprocessing is a crucial step in any Natural Language Processing pipeline. 
Raw textual data often contains inconsistencies, noise, and non-informative tokens 
that may negatively impact model performance.
</p>

<ul>
<li><b>Lowercasing:</b> All text was converted to lowercase to ensure that words such as 
"Government" and "government" are treated as identical features.</li>

<li><b>Stopword Removal:</b> Common words such as "the", "is", and "and" were removed 
using built-in English stopword lists. These words carry little discriminative value 
for topic classification.</li>

<li><b>No Stemming/Lemmatization:</b> Stemming was intentionally not applied in order 
to preserve interpretability and avoid distortion of meaningful domain-specific words 
(e.g., "election" vs "elect").</li>
</ul>

<p>
These preprocessing steps reduce vocabulary size and improve computational efficiency 
while maintaining important domain signals.
</p>

<hr>

<h3>2. Feature Extraction: TF-IDF Representation</h3>

<p>
Text documents cannot be directly processed by machine learning algorithms. 
Therefore, they must be converted into numerical feature vectors. 
This study uses <b>Term Frequency–Inverse Document Frequency (TF-IDF)</b>.
</p>

<p>
TF-IDF balances two important aspects:
</p>

<ul>
<li><b>Term Frequency (TF):</b> Measures how often a word appears in a document.</li>
<li><b>Inverse Document Frequency (IDF):</b> Reduces weight of words that appear in many documents.</li>
</ul>

<p>
Mathematically:
</p>

<p>
TF-IDF(word) = TF(word, document) × log(Total Documents / Documents containing word)
</p>

<p>
Advantages of TF-IDF:
</p>

<ul>
<li>Reduces dominance of very common words.</li>
<li>Emphasizes discriminative topic-specific terms.</li>
<li>Produces sparse, high-dimensional representations suitable for linear models.</li>
</ul>

<p>
However, TF-IDF does not capture semantic relationships or word order.
</p>

<hr>

<h3>3. Models Evaluated</h3>

<h4>3.1 Multinomial Naive Bayes (Generative Model)</h4>

<p>
Naive Bayes is a probabilistic classifier based on Bayes' Theorem with a 
conditional independence assumption between features.
</p>

<p>
It calculates:
</p>

<p>
P(Class | Document) ∝ P(Class) × Π P(Word | Class)
</p>

<p>
Advantages:
</p>

<ul>
<li>Extremely fast and efficient.</li>
<li>Performs well on high-dimensional sparse data.</li>
<li>Strong baseline for text classification.</li>
</ul>

<p>
Limitations:
</p>

<ul>
<li>Assumes word independence (often unrealistic).</li>
<li>Struggles when features are highly correlated.</li>
</ul>

<p>
In this project, Naive Bayes performs perfectly on BBC but slightly lower on AG News, 
indicating sensitivity to overlapping vocabulary.
</p>

<hr>

<h4>3.2 Logistic Regression (Discriminative Linear Model)</h4>

<p>
Logistic Regression is a linear classifier that directly models the posterior probability 
P(Class | Features) using a sigmoid function.
</p>

<p>
It optimizes log-loss (cross-entropy loss) to find the best decision boundary.
</p>

<p>
Advantages:
</p>

<ul>
<li>No independence assumption.</li>
<li>Handles correlated features better than Naive Bayes.</li>
<li>Produces well-calibrated probabilities.</li>
</ul>

<p>
Limitations:
</p>

<ul>
<li>Still linear — cannot capture non-linear relationships.</li>
<li>Performance depends heavily on feature representation.</li>
</ul>

<p>
Logistic Regression achieved the highest accuracy on AG News, suggesting 
better robustness in noisy and overlapping scenarios.
</p>

<hr>

<h4>3.3 Support Vector Machine (Margin-Based Classifier)</h4>

<p>
Linear Support Vector Machine (SVM) attempts to find the optimal hyperplane 
that maximizes the margin between classes.
</p>

<p>
Rather than modeling probabilities, SVM focuses on maximizing class separation.
</p>

<p>
Advantages:
</p>

<ul>
<li>Highly effective in high-dimensional sparse spaces.</li>
<li>Robust against overfitting in text classification tasks.</li>
<li>Often outperforms probabilistic models in NLP tasks.</li>
</ul>

<p>
Limitations:
</p>

<ul>
<li>Does not provide probability outputs by default.</li>
<li>Can be computationally expensive for very large datasets.</li>
</ul>

<p>
SVM performed very competitively across both datasets, confirming 
its suitability for text classification.
</p>

<hr>

<h3>4. Model Comparison & Justification</h3>

<table>
<tr>
<th>Model</th>
<th>Type</th>
<th>Strength</th>
<th>Weakness</th>
</tr>

<tr>
<td>Naive Bayes</td>
<td>Generative</td>
<td>Fast, simple, strong baseline</td>
<td>Independence assumption</td>
</tr>

<tr>
<td>Logistic Regression</td>
<td>Discriminative</td>
<td>Handles correlated features well</td>
<td>Linear boundary</td>
</tr>

<tr>
<td>SVM</td>
<td>Margin-based</td>
<td>Strong separation in sparse space</td>
<td>No probability output</td>
</tr>
</table>

<p>
Overall, results indicate that:
</p>

<ul>
<li>When classes are clearly separable (BBC), all linear models perform equally well.</li>
<li>When lexical overlap increases (AG News), discriminative models (Logistic Regression, SVM) show slight advantages.</li>
<li>Feature representation (TF-IDF) plays a critical role in overall performance.</li>
</ul>

<hr>

<h3>5. Evaluation Strategy</h3>

<ul>
<li><b>BBC Dataset:</b> 80–20 stratified train-test split to maintain class balance.</li>
<li><b>AG News:</b> Predefined train-test split used to ensure standardized evaluation.</li>
<li><b>Metrics:</b> Accuracy, Precision, Recall, F1-score.</li>
</ul>

<p>
Accuracy provides overall correctness, while Precision and Recall evaluate 
class-specific performance. F1-score balances both, especially important 
in slightly imbalanced datasets.
</p>

</div>


<!-- ================================================= -->
<div class="card">
<h2>Experimental Results</h2>

<h3>BBC Dataset Results</h3>
<table>
<tr><th>Model</th><th>Accuracy</th></tr>
<tr><td>Naive Bayes</td><td>1.000</td></tr>
<tr><td>Logistic Regression</td><td>1.000</td></tr>
<tr><td>SVM</td><td>1.000</td></tr>
</table>

<p>
All models achieved perfect accuracy due to strong lexical separation 
between categories. The decision boundary is nearly linearly separable 
in TF-IDF space.
</p>

<h3>AG News Dataset Results</h3>
<table>
<tr><th>Model</th><th>Accuracy</th></tr>
<tr><td>Naive Bayes</td><td>0.9739</td></tr>
<tr><td>Logistic Regression</td><td>0.9774</td></tr>
<tr><td>SVM</td><td>0.9761</td></tr>
</table>

<p>
Performance slightly decreased compared to BBC due to:
</p>

<ul>
<li>Shorter article length</li>
<li>Vocabulary overlap</li>
<li>Context ambiguity</li>
</ul>

<p>
Logistic Regression achieved the highest accuracy, suggesting that 
discriminative models better handle overlapping feature distributions.
</p>

</div>

<!-- ================================================= -->
<p>
The model comparison chart visually summarizes the performance of 
Naive Bayes, Logistic Regression, and Support Vector Machine across 
both datasets. The chart highlights two key observations:
</p>

<ul>
<li><b>Uniform Performance on BBC Dataset:</b> All three models achieved 
perfect accuracy (1.000). This confirms strong lexical separability 
between Sports and Politics in the BBC corpus. The feature space 
constructed using TF-IDF allows for near-linear decision boundaries, 
making even simple models highly effective.</li>

<li><b>Performance Variation on AG News Dataset:</b> Accuracy slightly 
decreases for all models (approximately 97–98%). This reflects the 
greater vocabulary overlap, shorter document length, and higher 
intra-class variability present in AG News.</li>
</ul>

<p>
Among the three models, <b>Logistic Regression</b> achieved the highest 
accuracy on AG News. This suggests that discriminative models may 
better handle overlapping feature distributions compared to generative 
models like Naive Bayes.
</p>

<p>
The <b>SVM</b> performed competitively, confirming its strength in 
high-dimensional sparse feature spaces. Meanwhile, <b>Naive Bayes</b> 
remained a strong baseline, demonstrating that probabilistic models 
are highly efficient for text classification tasks despite their 
independence assumption.
</p>

<p>
Overall, the comparison indicates that:
</p>

<ul>
<li>Feature representation (TF-IDF) plays a more significant role 
than classifier complexity in well-separated datasets.</li>

<li>In more challenging, real-world datasets, discriminative models 
provide marginal but consistent improvements.</li>

<li>The small accuracy gap (~0.3–0.4%) suggests all three models 
are robust and suitable for binary topic classification.</li>
</ul>

<p>
Therefore, the chart not only compares numerical accuracy but also 
illustrates how dataset characteristics influence classifier behavior.
</p>


<!-- ================================================= -->
<div class="card">
<h2>Discussion & Interpretation</h2>

<p>
The results demonstrate that classical machine learning models remain 
highly effective for structured topic classification tasks. 
However, dataset characteristics significantly impact performance.
</p>

<ul>
<li>BBC dataset confirms near-linear separability.</li>
<li>AG dataset demonstrates real-world ambiguity challenges.</li>
<li>Logistic Regression slightly outperforms due to discriminative optimization.</li>
<li>SVM performs consistently strong due to margin maximization.</li>
</ul>

<p>
The minimal difference between models suggests that feature representation 
(TF-IDF) plays a more critical role than classifier complexity for this task.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Challenges & Limitations</h2>

<ul>
<li>TF-IDF ignores semantic similarity and contextual meaning.</li>
<li>No word embeddings used (semantic relationships not captured).</li>
<li>Binary classification oversimplifies multi-topic articles.</li>
<li>No deep contextual modeling (e.g., transformers).</li>
<li>Domain shift between datasets affects generalization.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Future Improvements</h2>

<ul>
<li>Incorporate Word Embeddings (Word2Vec, GloVe).</li>
<li>Apply Transformer models (BERT, RoBERTa).</li>
<li>Use Cross-Validation for stronger evaluation.</li>
<li>Perform Error Analysis with confusion matrices.</li>
<li>Extend to multi-class classification.</li>
</ul>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Conclusion</h2>

<p>
This comparative study demonstrates that traditional linear classifiers 
combined with TF-IDF remain strong baselines for news topic classification. 
While performance is near-perfect in clean datasets, real-world data introduces 
ambiguity that slightly reduces accuracy.
</p>

<p>
The findings emphasize the importance of dataset characteristics, feature 
engineering, and model selection when designing text classification systems.
</p>

</div>

<!-- ================================================= -->
<div class="card">
<h2>Project Resources</h2>
<p>
<a href="Report.pdf" target="_blank">View Full Report (PDF)</a>
</p>
<p>
<a href="https://github.com/Vishaldhaniya08/Sports-vs-Politics-Text-Classification" target="_blank">
View Source Code on GitHub
</a>
</p>
</div>

<footer>
<p>© 2026 Vishal | Natural Language Understanding Assignment</p>
</footer>

</div>
</body>
</html>


